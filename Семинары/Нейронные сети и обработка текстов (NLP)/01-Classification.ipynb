{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификация текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном задании мы будем работать над задачей классификации текстов с использлванием различных методов векторизации слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "out_dict = dict()\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Предобработка текста и токенизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала скачаем исходные данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Считываем исходный набор данных\n",
    "df = pd.read_csv( \n",
    "    'https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv',\n",
    "    delimiter='\\t', # разделитель - символ табуляции\n",
    "    header=None     # заголовок отсутствует\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a stirring , funny and finally transporting re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apparently reassembled from the cutting room f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they presume their audience wo n't sit still f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is a visually stunning rumination on love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jonathan parker 's bartleby should have been t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0  a stirring , funny and finally transporting re...  1\n",
       "1  apparently reassembled from the cutting room f...  0\n",
       "2  they presume their audience wo n't sit still f...  0\n",
       "3  this is a visually stunning rumination on love...  1\n",
       "4  jonathan parker 's bartleby should have been t...  1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Посмотрим на первые 5 строк в считанном наборе данных\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6920"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       a stirring , funny and finally transporting re...\n",
       "1       apparently reassembled from the cutting room f...\n",
       "2       they presume their audience wo n't sit still f...\n",
       "3       this is a visually stunning rumination on love...\n",
       "4       jonathan parker 's bartleby should have been t...\n",
       "                              ...                        \n",
       "6915    painful , horrifying and oppressively tragic ,...\n",
       "6916    take care is nicely performed by a quintet of ...\n",
       "6917    the script covers huge , heavy topics in a bla...\n",
       "6918    a seriously bad film with seriously warped log...\n",
       "6919    a deliciously nonsensical comedy about a city ...\n",
       "Name: 0, Length: 6920, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предобработка входных данных достаточно проста и подробно описывается в комментариях к коду.\n",
    "\n",
    "Библиотека `nltk` широко используется при обработке текстов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В качестве обучающей выборки выбираем первые 5000 предложений\n",
    "texts_train = df[0].values[:5000]\n",
    "y_train = df[1].values[:5000]\n",
    "\n",
    "# В качестве тестовой выборки используем все оставшиеся предложения\n",
    "texts_test = df[0].values[5000:]\n",
    "y_test = df[1].values[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В качестве токенов будем использовать отдельные слова и знаки препинания\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenizer = WordPunctTokenizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предобработанный текст будем представлять в виде выделенных токенов, \n",
    "# разделённых пробелом\n",
    "\n",
    "preprocess = lambda text: ' '.join(tokenizer.tokenize(text.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: How to be a grown-up at work: replace \"I don't want to do that\" with \"Ok, great!\".\n",
      "after: how to be a grown - up at work : replace \" i don ' t want to do that \" with \" ok , great !\".\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим, как работает предобработка для заданной строки text\n",
    "\n",
    "text = 'How to be a grown-up at work: replace \"I don\\'t want to do that\" with \"Ok, great!\".'\n",
    "print(\"before:\", text,)\n",
    "print(\"after:\", preprocess(text),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how to be a grown - up at work : replace \" i don \\' t want to do that \" with \" ok , great !\".'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How to be a grown-up at work: replace \"I don\\'t want to do that\" with \"Ok, great!\".'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Примените функцию preprocess к каждому тексту\n",
    "texts_train = [preprocess(texts_train[i]) for i in range(len(texts_train))]\n",
    "texts_test = [preprocess(text) for text in texts_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выполняем небольшие проверки того, насколько корректно были обработаны тренировочная и тестовая выборки\n",
    "assert texts_train[5] ==  'campanella gets the tone just right funny in the middle of sad in the middle of hopeful'\n",
    "assert texts_test[74] == 'poetry in motion captured on film'\n",
    "assert len(texts_test) == len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующие функции помогут вам с визуализацией процесса обучения сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Не изменяйте блок кода ниже!\n",
    "\n",
    "# __________start of block__________\n",
    "def plot_train_process(train_loss, val_loss, train_accuracy, val_accuracy, title_suffix=''):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    axes[0].set_title(' '.join(['Loss', title_suffix]))\n",
    "    axes[0].plot(train_loss, label='train')\n",
    "    axes[0].plot(val_loss, label='validation')\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].set_title(' '.join(['Validation accuracy', title_suffix]))\n",
    "    axes[1].plot(train_accuracy, label='train')\n",
    "    axes[1].plot(val_accuracy, label='validation')\n",
    "    axes[1].legend()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_and_save_results(model, model_name, X_train, X_test, y_train, y_test, out_dict):\n",
    "    for data_name, X, y, model in [\n",
    "    ('train', X_train, y_train, model),\n",
    "    ('test', X_test, y_test, model)\n",
    "    ]:\n",
    "        if isinstance(model, BaseEstimator):\n",
    "            proba = model.predict_proba(X)[:, 1]\n",
    "        elif isinstance(model, nn.Module):\n",
    "            proba = model(X).detach().cpu().numpy()[:, 1]\n",
    "        else:\n",
    "            raise ValueError('Unrecognized model type')\n",
    "            \n",
    "        auc = roc_auc_score(y, proba)\n",
    "\n",
    "        out_dict['{}_{}'.format(model_name, data_name)] = auc\n",
    "        plt.plot(*roc_curve(y, proba)[:2], label='%s AUC=%.4f' % (data_name, auc))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], '--', color='black',)\n",
    "    plt.legend(fontsize='large')\n",
    "    plt.title(model_name)\n",
    "    plt.grid()\n",
    "    return out_dict\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Мешок слов "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Текст -- последовательность токенов, длина которой не фиксирована. Соответственно, если каждый текст представлять матрицей, где каждый столбец будет являться векторным описанием очередного токена, мы будем получать для разных текстов матрицы разного размера. Как мы знаем, такие матрицы непригодны для передачи в нейронные сети, поскольку размер входных данных сети - всегда фиксирован. Поэтому нужно научиться представлять любой текст некоторым тензором (вектором или матрицей) фиксированного размера.\n",
    "\n",
    "Самый простой способ -- воспользоваться классическим подходом к векторизации текстов: `мешком слов`. На лекции упоминалось, что в таком подходе каждый токен может быть закодирован разреженным вектором, размер которого будет равен размеру словаря. Соответственно, чтобы получить векторное представление всего текста, достаточно сложить векторные представления всех входящих в него токенов.\n",
    "\n",
    "Первое, что мы рассмотрим - `One-hot` кодирование токенов. В этом случае мы приходим к тому, что текст будет представлен вектором, длина которого равна размеру словаря, N-ым значением в этом векторе будет количество употребления слова с индексом N в исходном тексте.\n",
    "\n",
    "Для реализации такого подхода вы можете как воспользоваться `CountVectorizer` из `sklearn`, так и самостоятельно реализованный вариант. Обращаем ваше внимание, в данной задаче используется лишь k наиболее часто встречаемых слов из обучающей части выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 10000\n"
     ]
    }
   ],
   "source": [
    "# Отбираем только k наиболее популярных слов в текстах\n",
    "k = min(10000, len(set(' '.join(texts_train).split())))\n",
    "print(\"k =\", k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Построим словарь всех уникальных слов в обучающей выборке,\n",
    "# оставив только k наиболее популярных слов.\n",
    "\n",
    "counts = Counter(' '.join(texts_train).split()) \n",
    "bow_vocabulary = [key for key, val in counts.most_common(k)] # YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_bow(text):\n",
    "    \"\"\" \n",
    "        Функция, позволяющая превратить входную строку \n",
    "        в векторное представление на основании модели мешка слов. \n",
    "    \"\"\"\n",
    "    # Создайте массив нулей размерности словаря \n",
    "    sent_vec = np.zeros((len(bow_vocabulary))) # 10000\n",
    "\n",
    "    # Создайте словарь {word:count} в котором word - слово, count - встречаемость\n",
    "    counts = Counter(text.split())\n",
    "\n",
    "    # Положите в sent_vec соответствующие индексы значения counts\n",
    "    # так чтобы получился BOW\n",
    "    for i, token in enumerate(bow_vocabulary):\n",
    "        if token in counts:\n",
    "            sent_vec[i] = counts[token]\n",
    "    bow = np.array(sent_vec, 'float32')\n",
    "    \n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Векторизуйте ваши тексты в BOW с помощью выше написанной функции \n",
    "X_train_bow = np.stack(list(map(text_to_bow, texts_train))) #YOUR CODE\n",
    "X_test_bow = np.stack(list(map(text_to_bow, texts_test))) #YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Небольшие проверки\n",
    "k_max = len(set(' '.join(texts_train).split()))\n",
    "assert X_train_bow.shape == (len(texts_train), min(k, k_max))\n",
    "assert X_test_bow.shape == (len(texts_test), min(k, k_max))\n",
    "assert np.all(X_train_bow[5:10].sum(-1) == np.array([len(s.split()) for s in  texts_train[5:10]]))\n",
    "assert len(bow_vocabulary) <= min(k, k_max)\n",
    "assert X_train_bow[65, bow_vocabulary.index('!')] == texts_train[65].split().count('!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 2., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       [3., 1., 1., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'LogisticRegression' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m bow_model \u001b[38;5;241m=\u001b[39m LogisticRegression()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Обучаем\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m bow_model(X_train_bow, y_train)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'LogisticRegression' object is not callable"
     ]
    }
   ],
   "source": [
    "# Строим модель лог регрессии для полученных векторных представлений текстов\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "bow_model = LogisticRegression()\n",
    "\n",
    "# Обучаем\n",
    "bow_model(X_train_bow, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Что такое лог регрессия?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализируем\n",
    "out_dict = visualize_and_save_results(bow_model, 'bow_log_reg_sklearn', X_train_bow, X_test_bow, y_train, y_test, out_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Какие выводы можно сделать?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Логистическая регрессия в PyTorch\n",
    "\n",
    "Сейчас же реализуйте решение на основе логистической регрессии, но уже используя PyTorch. В результате вам должна быть доступна обученная модель, предсказывающая вероятности для двух классов. Качество на тестовой выборке должно не уступать логистической регрессии.\n",
    "\n",
    "Таким образом, нужно реализовать однослойную линейную сеть с использованием softmax в качестве функции активации и количеством выходов, равному количеству классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = # YOUR CODE\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = F.softmax(# YOUR CODE, dim=-1) \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# размер словаря\n",
    "input_dim = # YOUR CODE\n",
    "\n",
    "# количество классов \n",
    "output_dim = # YOUR CODE\n",
    "\n",
    "print(f\"input_dim = {input_dim}, output_dim = {output_dim}\")\n",
    "\n",
    "model = # YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не забывайте о функциях потерь: `nn.CrossEntropyLoss` объединяет в себе `LogSoftMax` и `NLLLoss`. \n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log\\left( \\frac{e^{x_{i, y_i}}}{\\sum_{j=1}^{C} e^{x_{i, j}}} \\right)\n",
    " -\\frac{1}{N} \\sum_{i=1}^{N} \\left( x_{i, y_i} - \\log \\sum_{j=1}^{C} e^{x_{i, j}} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "Также не забывайте о необходимости перенести тензоры на используемый `device`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Указываем лосс\n",
    "loss_function = # YOUR CODE\n",
    "\n",
    "# Какой оптимизатор используем?\n",
    "opt = # YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "X_train_bow_torch = torch.tensor(X_train_bow, dtype=torch.float32).to(device)\n",
    "X_test_bow_torch = torch.tensor(X_test_bow, dtype=torch.float32).to(device)\n",
    "\n",
    "y_train_torch = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "y_test_torch = torch.tensor(y_test, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция ниже поможет с обучением модели. Часть кода необходимо реализовать самостоятельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    opt,\n",
    "    X_train_torch,\n",
    "    y_train_torch,\n",
    "    X_val_torch,\n",
    "    y_val_torch,\n",
    "    n_iterations=500,\n",
    "    batch_size=32,\n",
    "    show_plots=True,\n",
    "    eval_every=50\n",
    "):\n",
    "    train_loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_loss_history = []\n",
    "    val_acc_history = []\n",
    "\n",
    "    local_train_loss_history = []\n",
    "    local_train_acc_history = []\n",
    "\n",
    "    # Цикл по итерациям\n",
    "    for # YOUR CODE\n",
    "\n",
    "        # Получаем случайный батч размера batch_size для проведения обучения\n",
    "        # Воспользуйтесь random.randint(low, high=None, size=None, dtype=int)\n",
    "        ix = # YOUR CODE\n",
    "        x_batch = # YOUR CODE\n",
    "        y_batch = # YOUR CODE\n",
    "\n",
    "        # Предсказываем отклик\n",
    "        y_predicted = # YOUR CODE\n",
    "        \n",
    "        # Вычисляем loss, как и выше\n",
    "        loss = # YOUR CODE\n",
    "\n",
    "        # Вычисляем градиенты\n",
    "        loss.backward() \n",
    "\n",
    "        # Adam step\n",
    "        opt.step()\n",
    "\n",
    "        # clear gradients\n",
    "        opt.zero_grad()\n",
    "\n",
    "        local_train_loss_history.append(loss.item())\n",
    "        local_train_acc_history.append(\n",
    "            accuracy_score(\n",
    "                y_batch.to('cpu').detach().numpy(),\n",
    "                y_predicted.to('cpu').detach().numpy().argmax(axis=1)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Делаем eval-шаг для каждой eval_every итерации\n",
    "        if # YOUR CODE\n",
    "            train_loss_history.append(np.mean(local_train_loss_history))\n",
    "            train_acc_history.append(np.mean(local_train_acc_history))\n",
    "            local_train_loss_history, local_train_acc_history = [], []\n",
    "\n",
    "            # Предсказания\n",
    "            predictions_val = # YOUR CODE\n",
    "            val_loss_history.append(loss_function(predictions_val, y_val_torch).to('cpu').detach().item())\n",
    "\n",
    "            acc_score_val = accuracy_score(y_val_torch.cpu().numpy(), predictions_val.to('cpu').detach().numpy().argmax(axis=1))\n",
    "            val_acc_history.append(acc_score_val)\n",
    "\n",
    "            if show_plots:\n",
    "                display.clear_output(wait=True)\n",
    "                plot_train_process(train_loss_history, val_loss_history, train_acc_history, val_acc_history)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучим на 3000 итераций\n",
    "bow_nn_model = # YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Не изменяйте блок кода ниже!\n",
    "# __________start of block__________\n",
    "out_dict = visualize_and_save_results(bow_nn_model, 'bow_nn_torch', X_train_bow_torch, X_test_bow_torch, y_train, y_test, out_dict)\n",
    "\n",
    "assert out_dict['bow_log_reg_sklearn_test'] - out_dict['bow_nn_torch_test'] < 0.01, 'AUC ROC on test data should be close to the sklearn implementation'\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь повторите процедуру обучения выше, но для различных значений k – размера словаря. \n",
    "\n",
    "В список`results` сохраните `AUC ROC` на тестовой части выборки для модели, обученной со словарем размера k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sizes_list = np.arange(100, 5800, 700)\n",
    "vocab_sizes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for k in vocab_sizes_list:\n",
    "    \n",
    "    # Отбор k наиболее популярных слов\n",
    "    # YOUR CODE\n",
    "\n",
    "    # Создание BOW\n",
    "    # YOUR CODE\n",
    "\n",
    "    # Переносим тензоры на device\n",
    "    X_train_bow_torch = torch.tensor(X_train_bow, dtype=torch.float32).to(device)\n",
    "    X_test_bow_torch = torch.tensor(X_test_bow, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Создание и обучение модели\n",
    "    # YOUR CODE\n",
    "    model = # YOUR CODE  # Заменяем input_dim на k\n",
    "    opt = # YOUR CODE\n",
    "    bow_nn_model = # YOUR CODE\n",
    "    \n",
    "    # Предсказание вероятностей для тестовой выборки\n",
    "    predicted_probas_on_test = bow_nn_model(X_test_bow_torch).detach().cpu().numpy()\n",
    "    auc = roc_auc_score(y_test, predicted_probas_on_test[:, 1])\n",
    "    results.append(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Не меняйте блок кода ниже!\n",
    "\n",
    "# __________start of block__________\n",
    "assert len(results) == len(vocab_sizes_list), 'Check the code above'\n",
    "assert min(results) >= 0.65, 'Seems like the model is not trained well enough'\n",
    "assert results[-1] > 0.84, 'Best AUC ROC should not be lower than 0.84'\n",
    "\n",
    "plt.plot(vocab_sizes_list, results)\n",
    "plt.xlabel('num of tokens')\n",
    "plt.ylabel('AUC')\n",
    "plt.grid()\n",
    "\n",
    "out_dict['bow_k_vary'] = results\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Использование TF-iDF признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для векторизации текстов также можно воспользоваться `TF-iDF`. Это позволяет исключить из рассмотрения многие слова, не оказывающие значимого влияния при оценке непохожести текстов.\n",
    "\n",
    "Ваша задача: векторизовать тексты используя TF-iDF (или TfidfVectorizer из sklearn, или реализовав его самостоятельно) и построить классификатор с помощью PyTorch.\n",
    "\n",
    "Затем также оцените качество классификации по AUC ROC для различных размеров словаря.\n",
    "\n",
    "Качество классификации должно быть не ниже 0.86 AUC ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(texts_train).toarray()\n",
    "X_test_tfidf = vectorizer.transform(texts_test).toarray()\n",
    "\n",
    "X_train_tfidf_torch = torch.tensor(X_train_tfidf, dtype=torch.float32).to(device)\n",
    "X_test_tfidf_torch = torch.tensor(X_test_tfidf, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train_tfidf.shape[1]\n",
    "output_dim = 2\n",
    "model = LogisticRegression(input_dim, output_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "model_tf_idf = train_model(model, opt, X_train_tfidf_torch, y_train_torch, X_test_tfidf_torch, y_test_torch, n_iterations=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Не меняйте блок кода ниже!\n",
    "\n",
    "# __________start of block__________\n",
    "out_dict = visualize_and_save_results(model_tf_idf, 'tf_idf_nn_torch', X_train_tfidf_torch, X_test_tfidf_torch, y_train, y_test, out_dict)\n",
    "\n",
    "assert out_dict['tf_idf_nn_torch_test'] >= out_dict['bow_nn_torch_test'], 'AUC ROC on test data should be better or close to BoW for TF-iDF features'\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sizes_list = np.arange(100, 5800, 700)\n",
    "results = []\n",
    "\n",
    "for k in vocab_sizes_list:\n",
    "\n",
    "    # Векторизация текстов с помощью TF-IDF\n",
    "    # YOUR CODE\n",
    "\n",
    "    # Переносим тензоры на device\n",
    "    X_train_tfidf_torch = torch.tensor(X_train_tfidf, dtype=torch.float32).to(device)\n",
    "    X_test_tfidf_torch = torch.tensor(X_test_tfidf, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Создание и обучение модели\n",
    "    model = # YOUR CODE\n",
    "    opt = # YOUR CODE\n",
    "    tf_idf_nn_model = # YOUR CODE\n",
    "\n",
    "    # Предсказание вероятностей для тестовой выборки\n",
    "    predicted_probas_on_test = tf_idf_nn_model(X_test_tfidf_torch).detach().cpu().numpy()\n",
    "    auc = roc_auc_score(y_test, predicted_probas_on_test[:, 1])\n",
    "    results.append(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Не меняйте блок кода ниже!\n",
    "\n",
    "# __________start of block__________\n",
    "assert len(results) == len(vocab_sizes_list), 'Check the code above'\n",
    "assert min(results) >= 0.65, 'Seems like the model is not trained well enough'\n",
    "assert results[-1] > 0.85, 'Best AUC ROC for TF-iDF should not be lower than 0.84'\n",
    "\n",
    "plt.plot(vocab_sizes_list, results)\n",
    "plt.xlabel('num of tokens')\n",
    "plt.ylabel('AUC')\n",
    "plt.grid()\n",
    "\n",
    "out_dict['tf_idf_k_vary'] = results\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Сравнение с Наивным Байесовским классификатором."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классические модели все еще способны показать хороший результат во многих задачах. Обучите наивный байесовский классификатор на текстах, векторизованных с помощью BoW и TF-iDF и сравните результаты с моделями выше.\n",
    "\n",
    "Комментарий: обращаем ваше внимание, необходимо выбрать подходящее к данной задаче априорное распределение для признаков, т.е. выбрать верную версию классификатора из sklearn: GaussianNB, MultinomialNB, ComplementNB, BernoulliNB, CategoricalNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Обучение наивного байесовского классификатора на BoW\n",
    "clf_nb_bow = MultinomialNB()\n",
    "clf_nb_bow.fit(X_train_bow, y_train)\n",
    "\n",
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "out_dict = visualize_and_save_results(clf_nb_bow, 'bow_nb_sklearn', X_train_bow, X_test_bow, y_train, y_test, out_dict)\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_nb_tfidf = MultinomialNB()\n",
    "clf_nb_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "out_dict = visualize_and_save_results(clf_nb_tfidf, 'tf_idf_nb_sklearn', X_train_tfidf, X_test_tfidf, y_train, y_test, out_dict)\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "assert out_dict['tf_idf_nb_sklearn_test'] > out_dict['bow_nb_sklearn_test'],' TF-iDF results should be better'\n",
    "assert out_dict['tf_idf_nb_sklearn_test'] > 0.86, 'TF-iDF Naive Bayes score should be above 0.86'\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Использование предобученных эмбеддингов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, воспользуемся предобученными эмбеддингами из библиотеки `gensim`. В нем доступно несколько эмбеддингов, предобученных на различных корпусах текстов. \n",
    "\n",
    "Напоминаем, что лучше использовать те эмбеддинги, которые были обучены на текстах похожей структуры.\n",
    "\n",
    "Ваша задача: обучить модель (достаточно логистической регрессии или же двуслойной неронной сети), используя усредненный эмбеддинг для всех токенов в отзыве, добиться качества не хуже, чем с помощью `BoW`/`TF-iDF` и снизить степень переобучения (разницу между `AUC ROC` на обучающей и тестовой выборках).\n",
    "\n",
    "**Обратите внимание!**\n",
    "\n",
    "В Google Colab установлена устаревшая версия библиотеки gensim, поэтому некоторые её обновлённые возможности могут не поддерживаться.\n",
    "\n",
    "Например, может возникать ошибка AttributeError.\n",
    "\n",
    "В этом случае нужно раскомментировать строку ниже, выполнить данную ячейку и повторно выполнить все последующие ячейки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "gensim_embedding_model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_average_embedding(text, gensim_embedding_model):\n",
    "    tokens = text.split()\n",
    "    embeddings = [gensim_embedding_model[token] for token in tokens if token in gensim_embedding_model]\n",
    "    if len(embeddings) > 0:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(gensim_embedding_model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_emb = [text_to_average_embedding(text, gensim_embedding_model) for text in texts_train]\n",
    "X_test_emb = [text_to_average_embedding(text, gensim_embedding_model) for text in texts_test]\n",
    "\n",
    "assert len(X_train_emb[0]) == gensim_embedding_model.vector_size, 'Seems like the embedding shape is wrong'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_emb_torch = torch.tensor(X_train_emb, dtype=torch.float32)\n",
    "X_test_emb_torch = torch.tensor(X_test_emb, dtype=torch.float32)\n",
    "\n",
    "y_train_torch = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_torch = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(gensim_embedding_model.vector_size, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "model = train_model(model, opt, X_train_emb_torch, y_train_torch, X_test_emb_torch, y_test_torch, n_iterations=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "\n",
    "out_dict = visualize_and_save_results(model, 'emb_nn_torch', X_train_emb_torch, X_test_emb_torch, y_train, y_test, out_dict)\n",
    "assert out_dict['emb_nn_torch_test'] > 0.87, 'AUC ROC on test data should be better than 0.86'\n",
    "assert out_dict['emb_nn_torch_train'] - out_dict['emb_nn_torch_test'] < 0.1, 'AUC ROC on test and train data should not be different more than by 0.1'\n",
    "# __________end of block__________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
