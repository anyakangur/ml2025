{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Токенизация: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первый шаг для задач NLP — это разбиение исходных данных на слова.\n",
    "Текст, с которым мы работаем, представлен в сыром формате: со всей пунктуацией и смайликами, присоединёнными к некоторым словам, поэтому простой str.split не подойдет.\n",
    "\n",
    "Давайте использовать `nltk` — библиотеку, которая решает множество задач NLP, таких как токенизация, стемминг или определение частей речи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade nltk gensim bokeh umap-learn numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import umap\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файл «./quora.txt» уже существует; не загружается.\n"
     ]
    }
   ],
   "source": [
    "# download the data:\n",
    "!wget \"https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1\" -O ./quora.txt -nc\n",
    "# alternative download link: https://yadi.sk/i/BPQrUu1NaTduEw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What TV shows or books help you read people's body language?\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = list(open(\"./quora.txt\", encoding=\"utf-8\"))\n",
    "data[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'TV', 'shows', 'or', 'books', 'help', 'you', 'read', 'people', \"'\", 's', 'body', 'language', '?']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "print(tokenizer.tokenize(data[50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приведите все к нижнему регистру и получите токены с помощью токенизатора. `data_tok` должен представлять собой список списков токенов для каждой строки в `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tok = # YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(data_tok[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(\n",
    "    isinstance(row, (list, tuple)) for row in data_tok\n",
    "), \"please convert each line into a list of tokens (strings)\"\n",
    "assert all(\n",
    "    all(isinstance(tok, str) for tok in row) for row in data_tok\n",
    "), \"please convert each line into a list of tokens (strings)\"\n",
    "is_latin = lambda tok: all(\"a\" <= x.lower() <= \"z\" for x in tok)\n",
    "assert all(\n",
    "    map(lambda l: not is_latin(l) or l.islower(), map(\" \".join, data_tok))\n",
    "), \"please make sure to lowercase the data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Word vectors: обучение векторных представлений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует не один способ обучения векторных представлений слов. Есть `Word2Vec` и `GloVe` с разными функциями потерь. Также есть `FastText`, который использует символьные модели для обучения эмбеддингов.\n",
    "\n",
    "Выбор огромен, поэтому давайте начнем с чего-то простого: `gensim` — это еще одна библиотека для NLP, которая включает множество векторных моделей, в том числе `word2vec`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот код создает и обучает модель Word2Vec для получения векторных представлений слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "model = Word2Vec(\n",
    "    data_tok,        # токенизированные данные\n",
    "    vector_size=32,  # размерность вектора эмбеддинга\n",
    "    min_count=5,     # учитывать слова, встречающиеся минимум 5 раз\n",
    "    window=5,        # размер окна контекста (5 слов вокруг целевого слова)\n",
    ").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Теперь вы можете получать векторные представления слов\n",
    "model.get_vector(\"anything\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Или напрямую запрашивать похожие слова. Поэкспериментируйте с этим!\n",
    "model.most_similar(\"bread\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "king = # YOUR CODE\n",
    "man = # YOUR CODE\n",
    "woman = # YOUR CODE\n",
    "result_vector = # YOUR CODE\n",
    "\n",
    "similar_words = model.similar_by_vector(result_vector, topn=10)\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Использование предобученной модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Долго же это заняло, да? А теперь представьте обучение полноразмерных (100~300 измерений) векторных представлений на гигабайтах текста: статьях из википедии или твитах.\n",
    "\n",
    "К счастью, сегодня вы можете получить предобученную модель векторных представлений всего в 2 строчки кода (без смс, обещаю)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "model = api.load(\"glove-twitter-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.key_to_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sort_by_descending_frequency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=[\"coder\", \"money\"], negative=[\"brain\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Визуализация векторных представлений слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один из способов проверить качество наших векторов - это построить их визуализацию. Проблема в том, что эти векторы находятся в 30+ мерном пространстве, а мы, люди, больше привыкли к 2-3 измерениям.\n",
    "\n",
    "К счастью, мы, специалисты по машинному обучению, знаем о методах __понижения размерности__.\n",
    "\n",
    "Давайте используем их для построения 1000 самых частых слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получите 1000 самых частоых слов\n",
    "\n",
    "# key_to_index - словарь в моделях gensim, который \n",
    "#   cопоставляет каждому слову (ключ - key) его числовой индекс (value - index)\n",
    "\n",
    "# keys - метод словаря в Python, который возвращает все ключи словаря\n",
    "\n",
    "words = # YOUR CODE\n",
    "print(words[::101])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # для каждого слова вычислите его вектор с помощью модели\n",
    "word_vectors = # YOUR CODE\n",
    "\n",
    "# Переведите list в numpy array с помощью np.asarray\n",
    "word_vectors = # YOUR CODE\n",
    "\n",
    "# Выведите размерность пооучившегося word_vectors\n",
    "print(# YOUR CODE )\n",
    "\n",
    "assert isinstance(word_vectors, np.ndarray)\n",
    "assert word_vectors.shape == (len(words), 25)\n",
    "assert np.isfinite(word_vectors).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear projection: PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Простейшим методом линейного снижения размерности является Метод Главных Компонент (PCA = Principial Component Analysis).\n",
    "\n",
    "В геометрической интерпретации PCA пытается найти оси, вдоль которых наблюдается наибольшая дисперсия данных. Если угодно, \"естественные\" оси.\n",
    "\n",
    "<img src=\"https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/pca_fish.png\" style=\"width:30%\">\n",
    "\n",
    "\n",
    "На математическом уровне метод пытается разложить объектно-признаковую матрицу $X$ на две матрицы меньшей размерности: $W$ и $\\hat W$, минимизируя среднеквадратичную ошибку:\n",
    "\n",
    "$$\\|(X W) \\hat{W} - X\\|^2_2 \\to_{W, \\hat{W}} \\min$$\n",
    "- $X \\in \\mathbb{R}^{n \\times m}$ - матрица объектов (центрированная);\n",
    "- $W \\in \\mathbb{R}^{m \\times d}$ - матрица прямого преобразования;\n",
    "- $\\hat{W} \\in \\mathbb{R}^{d \\times m}$ - матрица обратного преобразования;\n",
    "- $n$ объектов, $m$ исходных измерений и $d$ целевых измерений;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Стандартизовываем вектора, иначе PCA будет искажено признаками с большим масштабом.\n",
    "scaler = StandardScaler() \n",
    "word_vectors_pca = scaler.fit_transform(word_vectors)\n",
    "\n",
    "pca = PCA(2) # объект PCA, который будет сжимать данные до 2 компонент\n",
    "word_vectors_pca = pca.fit_transform(word_vectors_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert word_vectors_pca.shape == (\n",
    "    len(word_vectors),\n",
    "    2,\n",
    "), \"there must be a 2d vector for each word\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(word_vectors_pca[:, 0], word_vectors_pca[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не очень информативно..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте сделаем визуализацию красивее с помощью bokeh!\n",
    "Теперь график стал интерактивным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh.models as bm, bokeh.plotting as pl\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "output_notebook()\n",
    "def draw_vectors(\n",
    "    x,\n",
    "    y,\n",
    "    radius=10,\n",
    "    alpha=0.25,\n",
    "    color=\"blue\",\n",
    "    width=600,\n",
    "    height=400,\n",
    "    show=True,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"draws an interactive plot for data points with auxilirary info on hover\"\"\"\n",
    "    if isinstance(color, str):\n",
    "        color = [color] * len(x)\n",
    "    data_source = bm.ColumnDataSource({\"x\": x, \"y\": y, \"color\": color, **kwargs})\n",
    "\n",
    "    fig = pl.figure(active_scroll=\"wheel_zoom\", width=width, height=height)\n",
    "    fig.scatter(\"x\", \"y\", size=radius, color=\"color\", alpha=alpha, source=data_source)\n",
    "\n",
    "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
    "    if show:\n",
    "        pl.show(fig)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_vectors(word_vectors_pca[:, 0], word_vectors_pca[:, 1], token=words)\n",
    "\n",
    "# Наведите курсор мыши на точки и посмотрите, \n",
    "# сможете ли вы идентифицировать кластеры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Визуализация соседних точек с помощью UMAP\n",
    "PCA — это хороший метод, но он строго линейный и поэтому способен улавливать только общую высокоуровневую структуру данных.\n",
    "\n",
    "Если мы хотим сосредоточиться на сохранении близости соседних точек, мы можем использовать UMAP, который сам по себе является методом embedding. Здесь вы можете прочитать [подробнее о UMAP (на русском)](https://habr.com/ru/company/newprolab/blog/350584/) и о [t-SNE](https://distill.pub/2016/misread-tsne/), который также является методом embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = umap.UMAP(n_neighbors=5).fit_transform(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_vectors(embedding[:, 0], embedding[:, 1], token=words)\n",
    "\n",
    "# наведите курсор мыши на это место и посмотрите, \n",
    "# сможете ли вы идентифицировать кластеры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word2vec на PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Токенизация и формирование датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как вы уже могли заметить, идея, лежащая в основе [word2vec](https://arxiv.org/pdf/1310.4546), достаточно общая. В данном задании вы реализуете его самостоятельно.\n",
    "\n",
    "Дисклеймер: не стоит удивляться тому, что реализация от `gensim` (или аналоги) обучается быстрее и работает точнее. Она использует множество доработок и ускорений, а также достаточно эффективный код. Ваша задача добиться промежуточных результатов за разумное время.\n",
    "\n",
    "P.s. Как ни странно, GPU в этом задании нам не потребуется."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если вы работаете локально, выполните в выбранном окружении следующую команду:\n",
    "\n",
    "```pip install --upgrade nltk bokeh umap-learn```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade nltk bokeh umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "import string\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import umap\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "from tqdm.auto import tqdm as tqdma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the data:\n",
    "!wget https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1 -O ./quora.txt -nc\n",
    "# alternative download link: https://yadi.sk/i/BPQrUu1NaTduEw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(open(\"./quora.txt\", encoding=\"utf-8\"))\n",
    "data[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "print(tokenizer.tokenize(data[50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tok = [tokenizer.tokenize(\n",
    "        line.translate(str.maketrans(\"\", \"\", string.punctuation)).lower()\n",
    "    )\n",
    "    for line in data\n",
    "]\n",
    "len(data_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отфильтруйте данные, где меньше трех токенов в тексте. \n",
    "data_tok = # YOUR CODE\n",
    "len(data_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Несколько проверок:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(\n",
    "    isinstance(row, (list, tuple)) for row in data_tok\n",
    "), \"please convert each line into a list of tokens (strings)\"\n",
    "assert all(\n",
    "    all(isinstance(tok, str) for tok in row) for row in data_tok\n",
    "), \"please convert each line into a list of tokens (strings)\"\n",
    "is_latin = lambda tok: all(\"a\" <= x.lower() <= \"z\" for x in tok)\n",
    "assert all(\n",
    "    map(lambda l: not is_latin(l) or l.islower(), map(\" \".join, data_tok))\n",
    "), \"please make sure to lowercase the data\"\n",
    "\n",
    "assert all(len(x) >= 3 for x in data_tok), \\\n",
    "    \"Отфильтруйте данные, где меньше трех токенов в тексте\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже заданы константы ширины окна контекста и проведена предобработка для построения skip-gram модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = 5\n",
    "window_radius = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создайте словарь частот слов и множество уникальных слов, отфильтрованных по минимальной частоте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сначала разверните список списков токенов в плоский список токенов\n",
    "\n",
    "data_tok_1d = # YOUR CODE\n",
    "\n",
    "# Подсчитайте частоту каждого слов. Должен получится словарь {слово:частота}\n",
    "vocabulary_with_counter = # YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Взгляните что получилось\n",
    "vocabulary_with_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть ли вам что сказать по поводу результатов? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отфильтруйте токены, частота которых больше min_count\n",
    "\n",
    "word_count_dict = dict()\n",
    "for word, counter in vocabulary_with_counter.items():\n",
    "    if # YOUR CODE\n",
    "\n",
    "vocabulary = set(word_count_dict.keys())\n",
    "del vocabulary_with_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создайте словарь {слово:индекс}, где индекс - это просто порядковый номер\n",
    "word_to_index = # YOUR CODE\n",
    "\n",
    "# Создайте словарь {индекс:слово} \n",
    "index_to_word = # YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пары `(слово, контекст)` на основе доступного датасета сгенерированы ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cгенерируем контекстные пары для обучения Word2Vec (Skip-gram модели)\n",
    "\n",
    "context_pairs = []\n",
    "\n",
    "for text in data_tok:\n",
    "    for i, central_word in enumerate(text):\n",
    "        context_indices = range(\n",
    "            # YOUR CODE,  \n",
    "            # YOUR CODE\n",
    "        )\n",
    "        for j in context_indices:\n",
    "            if j == i:\n",
    "                continue\n",
    "            context_word = # YOUR CODE\n",
    "            if central_word in vocabulary and context_word in vocabulary:\n",
    "                context_pairs.append((\n",
    "                    # YOUR CODE, \n",
    "                    # YOUR CODE\n",
    "                ))\n",
    "\n",
    "print(f\"Generated {len(context_pairs)} pairs of target and context words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_pairs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Subsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы сгладить разницу в частоте встречаемсости слов, необходимо реализовать механизм subsampling'а.\n",
    "Для этого вам необходимо реализовать функцию ниже.\n",
    "\n",
    "Вероятность **исключить** слово из обучения (на фиксированном шаге) вычисляется как\n",
    "$$\n",
    "P_\\text{drop}(w_i)=1 - \\sqrt{\\frac{t}{f(w_i)}},\n",
    "$$\n",
    "где $f(w_i)$ – нормированная частота встречаемости слова, а $t$ – заданный порог (threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_frequent_words(word_count_dict, threshold=1e-5):\n",
    "    \"\"\"\n",
    "    Вычисляет вероятности субсэмплинга для слов на основе их частот.\n",
    "\n",
    "    Эта функция используется для определения вероятности сохранения слова в наборе данных\n",
    "    при субсэмплинге частых слов. Используемый метод вдохновлен подходом субсэмплинга\n",
    "    в Word2Vec, где частота каждого слова влияет на вероятность его сохранения.\n",
    "\n",
    "    Параметры:\n",
    "    - word_count_dict (dict): Словарь, где ключи - слова, а значения - их частоты.\n",
    "    - threshold (float, optional): Пороговый параметр, используемый для корректировки \n",
    "                                   частоты субсэмплинга слов. По умолчанию 1e-5.\n",
    "\n",
    "    Возвращает:\n",
    "    - dict: Словарь, где ключи - слова, а значения - вероятности сохранения каждого слова.\n",
    "\n",
    "    Пример:\n",
    "    >>> word_counts = {'the': 5000, 'is': 1000, 'apple': 50}\n",
    "    >>> subsample_frequent_words(word_counts)\n",
    "    {'the': 0.028, 'is': 0.223, 'apple': 1.0}\n",
    "    \"\"\"\n",
    "\n",
    "    total_count = # YOUR CODE \n",
    "    keep_prob_dict = {}\n",
    "\n",
    "    for word, count in word_count_dict.items():\n",
    "        f_w = # YOUR CODE \n",
    "        # Вероятность не может быть больше 1!\n",
    "        p_keep = # YOUR CODE \n",
    "        keep_prob_dict[word] = # YOUR CODE\n",
    "    return keep_prob_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob_dict = subsample_frequent_words(word_count_dict)\n",
    "assert keep_prob_dict.keys() == word_count_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Negative sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для более эффективного обучения необходимо не только предсказывать высокие вероятности для слов из контекста, но и предсказывать низкие для слов, не встреченных в контексте. Для этого вам необходимо вычислить вероятност использовать слово в качестве negative sample, реализовав функцию ниже.\n",
    "\n",
    "В оригинальной статье предлагается оценивать вероятность слов выступать в качестве negative sample согласно распределению $P_n(w)$\n",
    "$$\n",
    "P_n(w) = \\frac{U(w)^{3/4}}{Z},\n",
    "$$\n",
    "\n",
    "где $U(w)$ распределение слов по частоте (или, как его еще называют, по униграммам), а $Z$ – нормировочная константа, чтобы общая мера была равна $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative_sampling_prob(word_count_dict):\n",
    "    \"\"\"\n",
    "    Вычисляет вероятности негативного сэмплирования для слов на основе их частот.\n",
    "\n",
    "    Эта функция корректирует частоту каждого слова, возводя ее в степень 0.75, что\n",
    "    обычно используется в алгоритмах типа Word2Vec для уменьшения влияния очень частых слов.\n",
    "    Затем она нормализует эти скорректированные частоты, чтобы их сумма была равна 1,\n",
    "    формируя распределение вероятностей, используемое для негативного сэмплирования.\n",
    "\n",
    "    Параметры:\n",
    "    - word_count_dict (dict): Словарь, где ключи - слова, а значения - их частоты.\n",
    "\n",
    "    Возвращает:\n",
    "    - dict: Словарь, где ключи - слова, а значения - вероятности выбора каждого слова\n",
    "            для негативного сэмплирования.\n",
    "\n",
    "    Пример:\n",
    "    >>> word_counts = {'the': 5000, 'is': 1000, 'apple': 50}\n",
    "    >>> get_negative_sampling_prob(word_counts)\n",
    "    {'the': 0.298, 'is': 0.160, 'apple': 0.042}\n",
    "    \"\"\"\n",
    "\n",
    "    negative_sampling_prob_dict = {}\n",
    "\n",
    "     # 1. Вычисляем относительные частоты слов\n",
    "    total_count = # YOUR CODE\n",
    "    unigram_probs = # YOUR CODE\n",
    "\n",
    "    # 2. Возводим в степень 0.75\n",
    "    for word, prob in unigram_probs.items():\n",
    "        negative_sampling_prob_dict[word] = # YOUR CODE\n",
    "\n",
    "    # 3. Нормируем, чтобы сумма = 1\n",
    "    Z = # YOUR CODE\n",
    "    for word in negative_sampling_prob_dict:\n",
    "        negative_sampling_prob_dict[word] = # YOUR CODE\n",
    "    \n",
    "    return negative_sampling_prob_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_sampling_prob_dict = get_negative_sampling_prob(word_count_dict)\n",
    "assert negative_sampling_prob_dict.keys() == negative_sampling_prob_dict.keys()\n",
    "\n",
    "print(sum(negative_sampling_prob_dict.values()))\n",
    "assert np.allclose(sum(negative_sampling_prob_dict.values()), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для удобства, преобразуем полученные словари в массивы (т.к. все слова все равно уже пронумерованы)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob_array = np.array(\n",
    "    [keep_prob_dict[index_to_word[idx]] for idx in range(len(word_to_index))]\n",
    ")\n",
    "\n",
    "negative_sampling_prob_array = np.array(\n",
    "    [\n",
    "        negative_sampling_prob_dict[index_to_word[idx]]\n",
    "        for idx in range(len(word_to_index))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если все прошло успешно, функция ниже поможет вам с генерацией подвыборок (батчей)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_with_neg_samples(\n",
    "    context_pairs,\n",
    "    batch_size,\n",
    "    keep_prob_array,\n",
    "    word_to_index,\n",
    "    num_negatives,\n",
    "    negative_sampling_prob_array,\n",
    "):\n",
    "    batch = []\n",
    "    neg_samples = []\n",
    "\n",
    "    while len(batch) < batch_size:\n",
    "        center, context = random.choice(context_pairs)\n",
    "        if random.random() < keep_prob_array[center]:\n",
    "            batch.append((center, context))\n",
    "            neg_sample = np.random.choice(\n",
    "                range(len(negative_sampling_prob_array)),\n",
    "                size=num_negatives,\n",
    "                p=negative_sampling_prob_array,\n",
    "            )\n",
    "            neg_samples.append(neg_sample)\n",
    "    batch = np.array(batch)\n",
    "    neg_samples = np.vstack(neg_samples)\n",
    "    return batch, neg_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "num_negatives = 15\n",
    "batch, neg_samples = generate_batch_with_neg_samples(\n",
    "    context_pairs,\n",
    "    batch_size,\n",
    "    keep_prob_array,\n",
    "    word_to_index,\n",
    "    num_negatives,\n",
    "    negative_sampling_prob_array,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Наконец, время реализовать модель. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `nn.Embedding(num_embeddings, embedding_dim)` - создаёт таблицу эмбеддингов\n",
    "- `nn.init.xavier_uniform_` - инициализация нормальным распределением\n",
    "\n",
    "\n",
    "- `pos_scores` — скалярные dot-продукты для каждой пары (центральное слово, положительный контекст)\n",
    "- `neg_scores` — dot-продукты для каждой пары (центральное слово, отрицательные контексты). Используем bmm для батчевого умножения матриц\n",
    "- `torch.bmm(batch1, batch2)` — батчевое умножение матриц в PyTorch. Оно позволяет одновременно перемножать несколько пар матриц, что очень удобно для обработки батчей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напомним, что в случае negative sampling решается задача максимизации следующего функционала:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\log \\sigma({\\mathbf{v}'_{w_O}}^\\top \\mathbf{v}_{w_I}) + \\sum_{i=1}^{k} \\mathbb{E}_{w_i \\sim P_n(w)} \\left[ \\log \\sigma({-\\mathbf{v}'_{w_i}}^\\top \\mathbf{v}_{w_I}) \\right],\n",
    "$$\n",
    "\n",
    "где:\n",
    "- $\\mathbf{v}_{w_I}$ – вектор центрального слова $w_I$,\n",
    "- $\\mathbf{v}'_{w_O}$ – вектор слова из контекста $w_O$,\n",
    "- $k$ – число negative samplesЮ,\n",
    "- $P_n(w)$ – распределение negative samples, заданное выше,\n",
    "- $\\sigma$ – сигмоида."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.BCEWithLogitsLoss()` – это функция потерь в PyTorch, которая объединяет Sigmoid активацию + BCELoss (Binary Cross-Entropy) в одной оптимизированной функции. Используется для бинарной классификации (2 класса)\n",
    "\n",
    "$loss = - \\left[y \\cdot \\log(\\sigma(x)) + (1-y) \\cdot \\log(1-\\sigma(x)) \\right] = $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModelWithNegSampling(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.center_embeddings = None  # YOUR CODE HERE\n",
    "        self.context_embeddings = None  # YOUR CODE HERE\n",
    "\n",
    "    def forward(self, center_words, pos_context_words, neg_context_words):\n",
    "        # YOUR CODE HERE\n",
    "        pos_scores = 0  \n",
    "        neg_scores = 0 \n",
    "\n",
    "        return pos_scores, neg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_to_index)\n",
    "embedding_dim = 32\n",
    "num_negatives = 15\n",
    "\n",
    "model = SkipGramModelWithNegSampling(vocab_size, embedding_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.05)\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=150)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_counter = 0\n",
    "for weights in model.parameters():\n",
    "    params_counter += weights.shape.numel()\n",
    "assert params_counter == len(word_to_index) * embedding_dim * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_skipgram_with_neg_sampling(model,context_pairs,keep_prob_array,word_to_index,batch_size,num_negatives, negative_sampling_prob_array,steps,optimizer=optimizer,lr_scheduler=lr_scheduler,device=device,):\n",
    "    \n",
    "    pos_labels = torch.ones(batch_size).to(device)\n",
    "    neg_labels = torch.zeros(batch_size, num_negatives).to(device)\n",
    "    loss_history = []\n",
    "    for step in tqdma(range(steps)):\n",
    "\n",
    "        batch, neg_samples = generate_batch_with_neg_samples(\n",
    "            context_pairs, batch_size, keep_prob_array, word_to_index,\n",
    "            num_negatives, negative_sampling_prob_array)\n",
    "        \n",
    "        center_words = torch.tensor([pair[0] for pair in batch], dtype=torch.long).to(device)\n",
    "        pos_context_words = torch.tensor([pair[1] for pair in batch], dtype=torch.long).to(device)\n",
    "        neg_context_words = torch.tensor(neg_samples, dtype=torch.long).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pos_scores, neg_scores = model(center_words, pos_context_words, neg_context_words)\n",
    "\n",
    "        loss_pos = criterion(pos_scores, pos_labels)\n",
    "        loss_neg = criterion(neg_scores, neg_labels)\n",
    "\n",
    "        loss = loss_pos + loss_neg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_history.append(loss.item())\n",
    "        lr_scheduler.step(loss_history[-1])\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(\n",
    "                f\"Step {step}, Loss: {np.mean(loss_history[-100:])}, learning rate: {lr_scheduler._last_lr}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_skipgram_with_neg_sampling(\n",
    "    model,\n",
    "    context_pairs,\n",
    "    keep_prob_array,\n",
    "    word_to_index,\n",
    "    batch_size,\n",
    "    num_negatives,\n",
    "    negative_sampling_prob_array,\n",
    "    steps,\n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    device=device,\n",
    "):\n",
    "    pos_labels = torch.ones(batch_size).to(device)\n",
    "    neg_labels = torch.zeros(batch_size, num_negatives).to(device)\n",
    "    loss_history = []\n",
    "    for step in tqdma(range(steps)):\n",
    "\n",
    "        batch, neg_samples = generate_batch_with_neg_samples(\n",
    "            context_pairs, batch_size, keep_prob_array, word_to_index,\n",
    "            num_negatives, negative_sampling_prob_array)\n",
    "        \n",
    "        center_words = torch.tensor([pair[0] for pair in batch], dtype=torch.long).to(device)\n",
    "        pos_context_words = torch.tensor([pair[1] for pair in batch], dtype=torch.long).to(device)\n",
    "        neg_context_words = torch.tensor(neg_samples, dtype=torch.long).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pos_scores, neg_scores = model(center_words, pos_context_words, neg_context_words)\n",
    "\n",
    "        loss_pos = criterion(pos_scores, pos_labels)\n",
    "        loss_neg = criterion(neg_scores, neg_labels)\n",
    "\n",
    "        loss = loss_pos + loss_neg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_history.append(loss.item())\n",
    "        lr_scheduler.step(loss_history[-1])\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(\n",
    "                f\"Step {step}, Loss: {np.mean(loss_history[-100:])}, learning rate: {lr_scheduler._last_lr}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 2500\n",
    "batch_size = 512\n",
    "train_skipgram_with_neg_sampling(\n",
    "    model,\n",
    "    context_pairs,\n",
    "    keep_prob_array,\n",
    "    word_to_index,\n",
    "    batch_size,\n",
    "    num_negatives,\n",
    "    negative_sampling_prob_array,\n",
    "    steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, используйте полученную матрицу весов в качестве матрицы в векторными представлениями слов. Рекомендуем использовать для сдачи матрицу, которая отвечала за слова из контекста (т.е. декодера)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "_model_parameters = model.parameters()\n",
    "\n",
    "# Предполагаем, что первая матрица была предназначена для центрального слова\n",
    "embedding_matrix_center = next(_model_parameters).detach()  \n",
    "\n",
    "# Предполагаем, что вторая матрица была предназначена для контекстного слова\n",
    "embedding_matrix_context = next(_model_parameters).detach()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector(word, embedding_matrix, word_to_index=word_to_index):\n",
    "    return embedding_matrix[word_to_index[word]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Простые проверки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_1 = F.cosine_similarity(\n",
    "    get_word_vector(\"iphone\", embedding_matrix_context)[None, :],\n",
    "    get_word_vector(\"apple\", embedding_matrix_context)[None, :],\n",
    ")\n",
    "\n",
    "similarity_2 = F.cosine_similarity(\n",
    "    get_word_vector(\"iphone\", embedding_matrix_context)[None, :],\n",
    "    get_word_vector(\"dell\", embedding_matrix_context)[None, :],\n",
    ")\n",
    "\n",
    "print(f\"similarity_1 = {similarity_1}\")\n",
    "print(f\"similarity_2 = {similarity_2}\")\n",
    "\n",
    "assert similarity_1 > similarity_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_1 = F.cosine_similarity(\n",
    "    get_word_vector(\"windows\", embedding_matrix_context)[None, :],\n",
    "    get_word_vector(\"laptop\", embedding_matrix_context)[None, :],\n",
    ")\n",
    "similarity_2 = F.cosine_similarity(\n",
    "    get_word_vector(\"windows\", embedding_matrix_context)[None, :],\n",
    "    get_word_vector(\"macbook\", embedding_matrix_context)[None, :],\n",
    ")\n",
    "\n",
    "print(f\"similarity_1 = {similarity_1}\")\n",
    "print(f\"similarity_2 = {similarity_2}\")\n",
    "\n",
    "assert similarity_1 > similarity_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, взглянем на ближайшие по косинусной мере слова. Функция реализована ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(word, embedding_matrix, word_to_index=word_to_index, k=10):\n",
    "    word_vector = get_word_vector(word, embedding_matrix)[None, :]\n",
    "    dists = F.cosine_similarity(embedding_matrix, word_vector)\n",
    "    index_sorted = torch.argsort(dists)\n",
    "    top_k = index_sorted[-k:]\n",
    "    return [(index_to_word[x], dists[x].item()) for x in top_k.numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_nearest(\"python\", embedding_matrix_context, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также вы можете визуально проверить, как представлены в латентном пространстве часто встречающиеся слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 5000\n",
    "_top_words = sorted([x for x in word_count_dict.items()], key=lambda x: x[1])[\n",
    "    -top_k - 100 : -100\n",
    "]  # ignoring 100 most frequent words\n",
    "top_words = [x[0] for x in _top_words]\n",
    "del _top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = torch.cat(\n",
    "    [embedding_matrix_context[word_to_index[x]][None, :] for x in top_words], dim=0\n",
    ").numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh.models as bm\n",
    "import bokeh.plotting as pl\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "\n",
    "def draw_vectors(\n",
    "    x,\n",
    "    y,\n",
    "    radius=10,\n",
    "    alpha=0.25,\n",
    "    color=\"blue\",\n",
    "    width=600,\n",
    "    height=400,\n",
    "    show=True,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"draws an interactive plot for data points with auxilirary info on hover\"\"\"\n",
    "    if isinstance(color, str):\n",
    "        color = [color] * len(x)\n",
    "    data_source = bm.ColumnDataSource({\"x\": x, \"y\": y, \"color\": color, **kwargs})\n",
    "\n",
    "    fig = pl.figure(active_scroll=\"wheel_zoom\", width=width, height=height)\n",
    "    fig.scatter(\"x\", \"y\", size=radius, color=\"color\", alpha=alpha, source=data_source)\n",
    "\n",
    "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
    "    if show:\n",
    "        pl.show(fig)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = umap.UMAP(n_neighbors=5).fit_transform(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_vectors(embedding[:, 0], embedding[:, 1], token=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "import os\n",
    "import json\n",
    "\n",
    "assert os.path.exists(\n",
    "    \"words_subset.txt\"\n",
    "), \"Please, download `words_subset.txt` and place it in the working directory\"\n",
    "\n",
    "with open(\"words_subset.txt\") as iofile:\n",
    "    selected_words = iofile.read().split(\"\\n\")\n",
    "\n",
    "\n",
    "def get_matrix_for_selected_words(selected_words, embedding_matrix, word_to_index):\n",
    "    word_vectors = []\n",
    "    for word in selected_words:\n",
    "        index = word_to_index.get(word, None)\n",
    "        vector = [0.0] * embedding_matrix.shape[1]\n",
    "        if index is not None:\n",
    "            vector = embedding_matrix[index].numpy().tolist()\n",
    "        word_vectors.append(vector)\n",
    "    return word_vectors\n",
    "\n",
    "\n",
    "word_vectors = get_matrix_for_selected_words(\n",
    "    selected_words, embedding_matrix_context, word_to_index\n",
    ")\n",
    "\n",
    "with open(\"submission_dict.json\", \"w\") as iofile:\n",
    "    json.dump(word_vectors, iofile)\n",
    "print(\"File saved to `submission_dict.json`\")\n",
    "# __________end of block__________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
